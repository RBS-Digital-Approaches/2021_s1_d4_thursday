{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2021_s1_d4_m2b_image_thresholding.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyOTBAPwhsOHWkh9JJ/0hxYn"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"2beKMvl0-co-"},"source":["# Converting Images to Black and White for OCR\n","While Tesseract will to all appearances perform OCR quite happily on color images, it's worth noting that, behind the scenes, the software is performing a number of transformations on those images before attempting to recognize text, including converting our lovely color images to black and white. Most discussions I've seen of OCR workflows involve converting images before sending them to Tesseract in the first place. (If nothing else, the black and white files are considerably smaller. In a quick test, it looks like Tesseract completed text recognition of a black and white image in a bit less than half the time it took to process the same image in color.)\n","\n","The code in this notebook lets us see what different effects can be produced by adjusting parameters while converting color images to black and white.\n","\n","For most people, the focus of your attention doesn't really need to be on the details of the code, itself. Rather, focus on the different variables that affect the way that the images are transformed. I've set the notebook up so that you can change variables easily and re-process images to see what difference your changes make.\n"]},{"cell_type":"code","metadata":{"id":"RW7I14_-iGdG"},"source":["#Connect to Google Drive\n","from google.colab import drive\n","drive.mount('/gdrive')\n","\n","#Import libraries to allow interactive widgets in this notebook\n","import ipywidgets as widgets\n","from ipywidgets import interact, interact_manual, interactive"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vu_bP0HTaoWQ"},"source":["## Move images from Google Drive to Colaboratory environment\n","We won't use all of the images for this notebook, but it's more convenient just to have them in one place in the `data_class` folder, so we'll go ahead and pull them all in, anyway."]},{"cell_type":"code","metadata":{"id":"O6UgA40sa785"},"source":["%cp -r /gdrive/MyDrive/rbs_digital_approaches_2021/data_class/page_images/penn_pr3732_t7_1730b.zip /content/penn_pr3732_t7_1730b.zip\n","%cd /content/\n","!unzip penn_pr3732_t7_1730b.zip"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VUxKwY1ODmgf"},"source":["## Setting our source image\n","We'll use a page from the University of Pennsylvania's scan of one of the copies of James Thomson's *Sophonisba* in the Kislak Center for Special Collections, Rare Books, and Manuscripts (PR 3732 T7 1730b)."]},{"cell_type":"code","metadata":{"id":"cuK11EH4iifj"},"source":["source_directory = '/content/penn_pr3732_t7_1730b/'\n","source_image = source_directory + 'PR3732_T7_1730b_body0003.tif'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6uZZXFKPELMC"},"source":["## Converting images using PIL/Pillow\n","PIL was the original library for working with images in Python, but is not compatible with Python 3. It has been superseded by a fork called Pillow. Because PIL was so widely-used, however, Pillow was written as a drop-in replacement to maintain compatibility with existing code: we even import it by calling it `PIL` instead of `Pillow`. Pillow can do lots of useful things with images, and you can find lots of pointers online for using it, so we'll start there. (Note that Pillow is installed by default in Google Colaboratory. If you were working in a different environment, you'd need to install Pillow using `pip`.)\n","\n","We'll start by converting our color image to grayscale as an intermediate step towards getting our image to true black and white—a \"binary\" file in which each pixel is either black or white. We'll use [PIL's `convert()` method](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.convert), which requires us to select a mode for conversion. We'll use `L` for grayscale. (PIL's various modes are [explained in the documentation](https://pillow.readthedocs.io/en/stable/handbook/concepts.html#concept-modes).)"]},{"cell_type":"code","metadata":{"id":"gqenI-mDiJLZ"},"source":["# !pip install pillow\n","#Install the Image and ImageDraw libraries from PIL (actually Pillow)\n","from PIL import Image, ImageDraw"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ejBJVoGeQKCV"},"source":["### Converting from color to grayscale"]},{"cell_type":"code","metadata":{"id":"pFQB36XYiqE1"},"source":["#Use the open() method of Pillow's Image library to open the .tif file\n","pilcolor_image = Image.open(source_image)\n","\n","#Use the Image library's convert() method to convert the color image to grayscale\n","pilgray_image = pilcolor_image.convert('L')\n","\n","#Output\n","pilgray_image"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"avdYngYcG3Lv"},"source":["### Converting our grayscale image to black and white\n","Since converting from color to grayscale was as simple as selecting the mode `L`, let's try converting our grayscale image to binary by selecting mode `1`: \"1-bit pixels, black and white, stored with one pixel per byte.\""]},{"cell_type":"code","metadata":{"id":"mJP5AxQak30S"},"source":["pilbw_image = pilgray_image.convert('1')\n","pilbw_image"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"llwzJK5yIUDZ"},"source":["#### Overriding the default behavior: turning off dithering\n","Okay, that's not ideal. As noted in [the documentation](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.convert):\n",">The default method of converting a greyscale (“L”) or “RGB” image into a bilevel (mode “1”) image uses Floyd-Steinberg dither to approximate the original image luminosity levels. If dither is NONE, all values larger than 127 are set to 255 (white), all other values to 0 (black). To use other thresholds, use the point() method.\n","\n","Let's try that again, this time turning off the default dithering behavior: this time any pixel above (i.e., darker than) a threshold value of 127 will be turned to black and any pixel below that threshold will be turned to white."]},{"cell_type":"code","metadata":{"id":"urV_mOpcOEgs"},"source":["pilbw_image = pilgray_image.convert('1', dither=0)\n","pilbw_image"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rmRGuTcjPLKH"},"source":["#### Adjusting the threshold point manually\n","That's probably better, but some of the text seems kind of attenuated. Given how variable the inking can be in early print, this default value might not always work. Let's see what happens if we adjust the threshold point.\n","\n","Be sure to run the next cell to bring up a slider that will allow you to change the threshold value, then try experimenting with different threshold values.\n"]},{"cell_type":"code","metadata":{"id":"DxODAI8C9tXo"},"source":[" #@title Set a threshold value {display-mode: \"form\"}\n"," #@markdown Run this cell, then use the slider that will appear to adjust the threshold point for our image in the cell below. \n"," \n"," #@markdown You only need to run this cell once (re-running it will just set things back to the default value). Try adjusting the slider and then re-running the *next* cell a few times to see the difference that different threshold values make.\n"," thresh_value_slider = widgets.IntSlider(\n","    min=0,\n","    max=255,\n","    step=1,\n","    description='Threshold:',\n","    value=150\n",")\n"," display(thresh_value_slider)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NmfLczNdm2yD"},"source":["#See https://stackoverflow.com/questions/9506841/using-python-pil-to-turn-a-rgb-image-into-a-pure-black-and-white-image/50090612#50090612\n","\n","#Get the current value of our slider widget from the cell above\n","thresh = thresh_value_slider.value\n","\n","#This is defining a kind of quickie function 'fn' that will be used at line 18\n","#This is going to look at every pixel of our image. If the value of that pixel is\n","#greater than our \"thresh\" value (set by the slider above), then set the pixel to\n","#255 (pure black). If the pixel value is less than \"thresh,\" set the pixel to 0\n","#(pure white) \n","fn = lambda x : 255 if x > thresh else 0\n","\n","#Convert our image, this time overriding the default dithering behavior\n","#with the threshold we've chosen, using the lambda function from line 11\n","pilbinary_image = pilgray_image.convert('L').point(fn, mode='1')\n","\n","pilbinary_image"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sZTgwhX8RZua"},"source":["It shouldn't take much experimenting to see that different threshold points can create very different results. But let's say we have 1,000 page images from 50 different books. Given the variability of early print, there may not be one perfect threshold value that gets the best results for all of our images: what would be great for one image might leave another one too dark and noisy, and might leave a third too faint.\n","\n","What we need is a way to determining a good threshold value for each image without having to experiment on each image individually. Fortunately, this is a problem that people have worked on. While we might be able out how to implement, say [Otsu's method](https://en.wikipedia.org/wiki/Otsu%27s_method) in code ourselves, we'd probably be better off taking advantage of the fact that other people have already done that work. For this, we'll turn from PIL to a different library."]},{"cell_type":"markdown","metadata":{"id":"RnAXVGtvQzHL"},"source":["## Converting images using OpenCV\n","OpenCV is a computer vision library that can be used for all sorts of things, including feature detection, image classification, and more. A library like this one might seem to be overkill for simply converting images from color to black and white, but it offers us lots of things built-in that would be difficult to work out from scratch, ourselves.\n","\n","(Note that OpenCV and the Python wrapper for it are installed by default in Google Colab. If you were working in a different environment, you'd first need to install OpenCV—the process differs depending on your operating system, so I won't go into that here. You'd also need to install the Python wrapper using `pip`.)"]},{"cell_type":"code","metadata":{"id":"_DmvvP1xoXIN"},"source":["# !pip install opencv-python\n","import cv2\n","#This is a Google Colab-specific patch to enable us to view OpenCV images\n","#in the browser\n","from google.colab.patches import cv2_imshow\n","#Other Python libraries that CV2 needs\n","import matplotlib.pyplot as plt\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vURvfYpdWiQr"},"source":["#### Converting from color to grayscale\n","Opening and converting images with OpenCV is more or less like opening and converting images with PIL, but you'll notice a few differences. We use `cv2.imread()` and `cv2.cvtColor` rather than `Image.open()` and `Image.convert`, for example, and, in addition to supplying the file to open and convert, we also have to indicate a method related to the color space we need to work in. (We're working with a Python wrapper for OpenCV, but OpenCV, itself, is written in C++, so some of the conventions around capitalization and so forth will look different from lots of Python code you'll see.)"]},{"cell_type":"code","metadata":{"id":"ocgejZSsh_CE"},"source":["#Open the color image\n","cv2color_image = cv2.imread(source_image, cv2.IMREAD_COLOR)\n","\n","#Convert the color image to grayscale\n","cv2gray_image = cv2.cvtColor(cv2color_image, cv2.COLOR_BGR2GRAY)\n","\n","#Output\n","cv2_imshow(cv2gray_image)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O1Yi51WyYqIh"},"source":["Yep. Looks like a grayscale image, all right. (And the conversion was actually rather faster than with PIL.)"]},{"cell_type":"markdown","metadata":{"id":"rkD4A9imY29h"},"source":["#### Converting from grayscale to black and white\n","We could simply convert our grayscale image to black and white right now, but there are arguments for applying a slight blur to our image first. While it seems counterintuitive that we would want to make an image blurrier when what we want to do is to recognize text clearly, that blurring can help to minimize the effect of any noise in the image (including, say, tiny flecks in the paper).\n","\n","(Be sure to run the next cell, as it creates a widget for adjusting the blur that we'll apply in the cell following it.)"]},{"cell_type":"code","metadata":{"id":"QTBPamUOvPct"},"source":["#@title Set values for Gaussian blur {display-mode: \"form\"}\n","#@markdown Try adjusting the value that will be used for blurring in the next cell.\n","\n","#@markdown (You only need to run this cell once—re-running it will simply reset it to the default value. After changing the value of the slider, try re-running the cell below this one.)\n","blur = widgets.IntSlider(min=1, max=31, step=2, value=5, description='Blur')\n","display(blur)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EJyKWkxkcxdf"},"source":["#Apply a Gaussian blur, using a kernel whose width and height are both equal to\n","#the value set by the slider above.\n","cv2blurred_image = cv2.GaussianBlur(cv2gray_image, (blur.value, blur.value), 0)\n","cv2_imshow(cv2blurred_image)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fqUncz6da17z"},"source":["#### Determining an appropriate threshold using Otsu's method \n","We'll experiment with two different methods for automatically thresholding our image. The first, Otsu's method (as best I understand without actually being able to follow the equations) arrives at a threshold level for the image as a whole by noting the distribution of intensities across *all* the pixels of an image and finding the threshold level that optimally divides those intensities into two clusters: the point at which it makes most sense to say \"Everything greater than this belongs together in one group, and everything less than this belongs together in a different group.\" The \"greater than\" group gets turned to black, and the \"less than\" group gets turned to white. \n","\n","This is more or less what we were trying to do experimentally, ourselves, with the threshold slider, above, but without the trial and error. Otsu's method seems to work quite well for the kinds of page images we're dealing with, as we'll see in the cell below.\n","\n","**Note:** Make sure that the blurred image you've created in the cell above looks good to you, since that's what we'll be thresholding here. If you had experimented with the blur until it started looking terrible, now would be the time to set it back to a more sensible level."]},{"cell_type":"code","metadata":{"id":"M1BhOhh9iZhC"},"source":["#Threshold the blurred image using Otsu's method. This line looks a little weird.\n","#We're really creating two variables at the same time--T and \n","#cv2binary_otsu_image--grouped together as a tuple. The cv2.threshold() method\n","#will return both the threshold level that's calculated by Otsu's method (that \n","#will be T) and the image that results from applying that threshold. If we\n","#just want the image, we could substitute the following:\n","#cv2binary_otsu_image = cv2.threshold(cv2blurred_image, 0, 255, cv2.THRESH_OTSU)[1]\n","(T, cv2binary_otsu_image) = cv2.threshold(cv2blurred_image, 0, 255, cv2.THRESH_OTSU)\n","\n","#Output\n","cv2_imshow(cv2binary_otsu_image)\n","print('Otsu threshold is: ' + str(T))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-TU_4l50fWUt"},"source":["That looks pretty good with a threshold of 155. How does that compare to the threshold level you had arrived at, above?\n","\n","Note that that this conversion was slower than what we got when supplying PIL with a threshold level, but we can be more confident that this method has found a good threshold level. If we were trying to automate the conversion of scores (or hundreds, or thousands) of images, the tradeoff in speed for adaptability would almost surely be worth it."]},{"cell_type":"markdown","metadata":{"id":"O8Pzv3IDgBFk"},"source":["#### Sounds great! Is there a catch?\n","Otsu's method seems to work great for this page image. We could imagine circumstances, though, where the results wouldn't be so good. In an image where the separation between light and dark pixels was less clear, the threshold determined by Otsu's method might yield a result that was difficult to read. This could be the case, for instance, with an image of a page with lots of ink showing through from the other side, or with too much shadow on the page from less-than-ideal photographic circumstances, or, as we'll see in the next cell, with severe foxing of the paper."]},{"cell_type":"code","metadata":{"id":"2AR2m_cCj9jL"},"source":["foxed = source_directory + 'st_tz_foxing.jpg'\n","color_foxed = cv2.imread(foxed, cv2.IMREAD_COLOR)\n","cv2_imshow(color_foxed)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KFgYo8JZvfZU"},"source":["How does Otsu's method do with this image? Wellll...\n","\n","(Note that this is just an image I found in a [blog post from the New England Document Conservation Center](https://www.nedcc.org/about/nedcc-stories/story-tz-interview), and not an image that was created through careful digitization. Still, if you've spent any time working with scanned books, you've surely seen something like this before.)"]},{"cell_type":"code","metadata":{"id":"6nXWXaKqvlWv"},"source":["gray_foxed = cv2.cvtColor(color_foxed, cv2.COLOR_BGR2GRAY)\n","blurred_foxed = cv2.GaussianBlur(gray_foxed, (5, 5), 0)\n","otsu_foxed = cv2.threshold(blurred_foxed, 0, 255, cv2.THRESH_OTSU)[1]\n","\n","cv2_imshow(otsu_foxed)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wd_k0IZQm83k"},"source":["#### Applying adaptive thresholding for problematic images\n","Rather than attempting to calculate a single threshold point appropriate for the image as a whole, adpative thresholding first divides the image into segments based on the levels in different regions of the image and then calculates a separate threshold point for each segment. For a generally good image like our pages of *Sophonisba*, the difference isn't really all that noticeable. We can certainly see a difference, but it's not terribly dramatic."]},{"cell_type":"code","metadata":{"id":"1DBFujh6oXre"},"source":["cv2binary_adaptive_image = cv2.adaptiveThreshold(cv2blurred_image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 101, 30)\n","cv2_imshow(cv2binary_adaptive_image)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qr_oox3JoeoH"},"source":["But it makes a remakable difference for the badly foxed title page we saw giving Otsu's method trouble:"]},{"cell_type":"code","metadata":{"id":"WTQuRUDTwUEZ"},"source":["cv2binary_adaptive_image = cv2.adaptiveThreshold(blurred_foxed, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 101, 30)\n","cv2_imshow(cv2binary_adaptive_image)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jhdTreYFdXZt"},"source":["## Clear Google Colab environment\n","I don't *think* that leaving all those .tif images in the Colab environment will count against your Google Drive storage quota, but it might, so let's just wipe out all of those files"]},{"cell_type":"code","metadata":{"id":"XZ4Lmu5VdlMg"},"source":["%cd /content/\n","!rm -r ./*"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xO8U-v1pzTQ5"},"source":["## Takeaways\n","During the course of OCR, images are going to get converted to black and white, and the quality of that conversion can have a dramatic effect on the quality of the recognized text.\n","\n","Different images respond better to different treatments. You could set a manual threshold level of say 150 or 155 and get pretty good results most of the time—but sometimes the results would not be good at all. \n","\n","Automated methods for figuring out likely-optimal values for thresholding any given image are necessary to do this kind of work at scale, but here, too, there are options. All-around, Otsu's method is quite effective (and since it's faster than adaptive thresholding, it's probably a good choice for most uses). But there will be some occasions where it's not the best tool for the job."]}]}