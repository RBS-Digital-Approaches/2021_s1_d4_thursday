{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TesseractTrainVanillaInstall.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyOPejQRosBA5s5XVIwjLq6f"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"puCwmqeG0ajS"},"source":["# Training Tesseract\n","As has been the case with so many things this week, the process of getting ready for OCR training is far more involved than the training, itself—if you've gotten everything set up correctly, you mostly just start a process running, wait for it to complete, start another process, wait, etc.\n","\n","The training we're doing here wouldn't take the days or weeks that Carl mentioned yesterday when talking about the kinds of big neural net models that it's possible to get into. (Apparently, though, training Tesseract from scratch on a very large data set—like the hundreds of thousands of pages that were used to train Tesseract's standard English language model—can, indeed, take weeks). \n","\n","I'm recommending that you work through the code *up to but not including* the cell that would build the full training so that you can see the steps that go into getting all the data ready to build into a completed \"language\" for Tesseract. (That cell seems to take almost exactly an hour to complete the default 10,000 iterations.) I've put all the files that are need to prepare the training in our `class_data` folder (these are the same files that are produced by the previous workbooks, with—I think—two hand-tweaks). So you can step through the process to see how it works. (And, if you really want to see the whole thing happen, you would be able to run the cell that performs the training.) \n","\n","I've also made the output of a completed training run available in the `class_data` folder. We can use that to test out different results: Tesseract allows us to continue a training from a specified checkpoint, so we can experiment with what how the model does with varying numbers of iterations (up to 15,000).\n","\n","We'll use a shell script called `tesstrain` provided by the maintainers of Tesseract to handle the training. This isn't the only way to train Tesseract, by any means, but it's a relatively simple one: we provide the necessary data, and `tesstrain` takes care of firing off all the various commands to Tesseract.\n","\n","In this notebook, we'll be stepping away from Python temporarily and working largely in the Unix `bash` shell, so the commands you'll be seeing have more in common with our routines for getting everything set up for the class (`mkdir`, `cd`, `ls`, etc.) than anything else. \n","\n","(If you're familiar with working at the command line, you may wonder why I'm constantly using `cd`, even if you're already in the directory if you're stepping through the cells in order. I went that slightly paranoid route because Jupyter notebooks lend themselves to being run *out* of order, so I've tried to make sure that any command was immediately preceded by a change into the correct directory.)"]},{"cell_type":"markdown","metadata":{"id":"arxCRwJpRfo4"},"source":["## Connect to Google Drive"]},{"cell_type":"code","metadata":{"id":"tx_YKdgcVd7v"},"source":["from google.colab import drive\n","drive.mount('/gdrive')\n","from google.colab import files"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iWupx0CZRkHn"},"source":["## Install Tesseract and another library that tesstrain.sh needs"]},{"cell_type":"code","metadata":{"id":"CAovPzC1TEwW"},"source":["! apt install tesseract-ocr\n","! apt install bc"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VJf3CIl3UIHM"},"source":["## Get preliminary files\n","We'll clone `tresstrain.sh` from GitHub and also copy the pre-cooked materials for training from Google Drive into the Colaboratory environment. We're going to circle back to some of these files in a bit, but for now we'll just move the `sophonisba-ground-truth` folder to the place where `tesstrain` expects it to be."]},{"cell_type":"code","metadata":{"id":"Oh5rIig1UO6q"},"source":["#Clone tesstrain repo\n","%cd /content/\n","! git clone https://github.com/tesseract-ocr/tesstrain\n","\n","#Make expected data directory in tesstrain directory\n","%cd /content/tesstrain/\n","%mkdir data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a-0HN3rpWx_G"},"source":["%cd /content/\n","#Copy prepared training materials from Google Drive to /content for now\n","%cp /gdrive/MyDrive/L-100a/ocr_training_materials.zip /content/ocr_training_materials.zip\n","!unzip /content/ocr_training_materials.zip\n","\n","#Move sophonisba-ground-truth (line-level images and text) to tesstrain/data\n","%mv /content/ocr_training_materials/sophonisba-ground-truth/ /content/tesstrain/data/sophonisba-ground-truth "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DpBbRzOlXbHU"},"source":["%cd /content/ocr_training_materials/\n","%ls"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IU5F8YF7VEg1"},"source":["## Using tesstrain to create a proto-model for our new Tesseract training\n","Let's first have a look at the `tesstrain` directory: it's full of scripts to automate the process of training Tesseract. We'll trust that these people know what they're doing."]},{"cell_type":"code","metadata":{"id":"m5lJY7ix3uBN"},"source":["%cd /content/tesstrain\n","%ls"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UVVbOJzUVbkB"},"source":["### Create the skeleton of our new training\n","Tesstrain will generate a list of unicode characters associated with our ground truth files, as well as some other scaffolding for our model, which we'll call `sophonisba`. This will take several minutes. Expect to see some non-fatal errors reported at the end. Fingers crossed that they won't hinder us too much."]},{"cell_type":"code","metadata":{"id":"JIdXedCI4HcK"},"source":["!make unicharset lists proto-model MODEL_NAME=sophonisba"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6J1gUgzQV1qW"},"source":["### Let's see what tesstrain created"]},{"cell_type":"code","metadata":{"id":"5sK6F29S9ZFR"},"source":["%cd /content/tesstrain/data/sophonisba/\n","%ls"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GvJcsbBTWHeH"},"source":["## Extending Tesseract's existing English model, rather than starting from scratch\n","In theory, we could train Tesseract with just the line images and transcriptions from *Sophonisba*. (Well, we could do it in practice, too—I did it that way last week and can report that it works. -Ish.) But that's really much too small a base on which to ground an entire language model. \n","\n","There are certainly cases where it makes sense to think about building a model from scratch (for a language that Tesseract doesn't currently support, for instance, or for an especially unusual typeface).\n","\n","Without more text than we have, though, we're almost surely better off \"fine tuning\" Tesseract: the documentation notes that it's possible to get fairly good results here even without a lot of training data. That seems like our best bet.\n","\n","To do that, we'll have to extract some information from Tesseract's existing English language model. This involves several terminal commands that are specific to Tesseract."]},{"cell_type":"code","metadata":{"id":"utVLR4_6AauS"},"source":["#Create a new directory to hold a copy of Tesseract's English language model and\n","#copy that model from its location in the system's installation of Tesseract to \n","#a folder in /content\n","!mkdir /content/tesstrain/data/eng/\n","%cp /usr/share/tesseract-ocr/4.00/tessdata/eng.traineddata /content/tesstrain/data/eng/eng.traineddata\n","\n","#Move to the directory with our copy of the English language model and extract\n","#several components using combine_tessdata (-e is for \"extract\")\n","%cd /content/tesstrain/data/eng/\n","!combine_tessdata -e eng.traineddata eng.lstm-unicharset eng.lstm-word-dawg eng.lstm-punc-dawg   eng.lstm-number-dawg"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eCmmNblHYiAH"},"source":["### Extract the English language model's word list and punctuation rules list\n","Tesseract's word lists are in the form of \"Directed Acyclic Word Graphs,\" or \"DAWG files.\" ([This blog post](http://stevehanov.ca/blog/?id=115) provides an explanation with illustrations.) If we want to do anything, we need to get the information out of those graphs using `dawg2wordlist`. We'll extract the English word list and punctuation list."]},{"cell_type":"code","metadata":{"id":"f4MjbSfJDNMi"},"source":["%cd /content/tesstrain/data/eng/\n","!dawg2wordlist eng.lstm-unicharset eng.lstm-word-dawg english_words.txt\n","!dawg2wordlist eng.lstm-unicharset eng.lstm-punc-dawg engpunclist.txt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"upWFzmIgZUd2"},"source":["#### Let's have a look...\n","Yep. There are text files there now, all right."]},{"cell_type":"code","metadata":{"id":"n-Q7s_weFx9K"},"source":["%cd /content/tesstrain/data/eng/\n","%ls"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6ldj75YVZbvT"},"source":["### Add words and punctuation patterns from ECCO to Tesseract's existing lists\n"]},{"cell_type":"code","metadata":{"id":"Z7WlD-55VtJS"},"source":["!cat /content/tesstrain/data/eng/english_words.txt /content/ocr_training_materials/ecco-words.txt | sort | uniq > /content/testing_pipe.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F7ouL0hhV_n8"},"source":["%cd /content/tesstrain/data/\n","%ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CZXQnbtLGKtJ"},"source":["#Copy ECCO word and punctuation lists\n","!mv /content/ocr_traning_materials/ecco-words.txt /content/tesstrain/data/ecco-words.txt\n","!mv /content/ocr_training_materials/ecco-punct.txt /content/tesstrain/data/ecco-punct.txt\n","%cd /content/tesstrain/data/\n","\n","#Concatenate Tesseract's English word list with our ecc-words, then sort the \n","#resulting file and eliminate duplicate lines. Note: this is *literally* a \n","#\"pipeline\": we send the output of one command to the next with the pipe character\n","#(\"|\") before saving the output as a file\n","!cat /content/tesstrain/data/eng/english_words.txt /content/ocr_training_materials/ecco-words.txt | sort | uniq > combined-words-sorted-unique.txt\n","\n","#Do the same thing for the punctuation lists\n","!cat /content/tesstrain/data/eng/engpunclist.txt /content/ocr_training_materials/ecco-punct.txt | sort | uniq > combined-punc-sorted-unique.txt\n","\n","#See what we have\n","%ls"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fKMwAADXajrq"},"source":["#### Turning our text files into DAWG files\n","Just as we used `dawg2wordlist` to unpack Tesseract's DAWG files into plain text, we now need to use the complementary `wordlist2dawg` to turn our plain text files into DAWG files that Tesseract can use."]},{"cell_type":"code","metadata":{"id":"28wINf-0q78A"},"source":["!wordlist2dawg /content/tesstrain/data/combined-words-sorted-unique.txt /content/tesstrain/data/sophonisba/sophonisba.wordlist /content/tesstrain/data/sophonisba/sophonisba.unicharset\n","!wordlist2dawg /content/tesstrain/data/combined-punc-sorted-unique.txt /content/tesstrain/data/sophonisba/sophonisba.punc /content/tesstrain/data/sophonisba/sophonisba.unicharset"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IBxzwj8lbFIX"},"source":["### Let's take a quick look at the files we've added to sophonisba\n","These are the files that we'll be telling `tesstrain` about as it executes the Tesseract training routine."]},{"cell_type":"code","metadata":{"id":"AKqV3gJOJzMy"},"source":["%cd /content/tesstrain/data/sophonisba/checkpoints\n","%ls -lt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3ff62dGbAFGz"},"source":["## DO NOT RUN THIS CELL\n","Unless you want to kick off the hour-long training"]},{"cell_type":"code","metadata":{"id":"xPv1rzO6aFZa"},"source":["#Start training. Go eat a sandwich, or something, 'cause this will take an hour\n","#When it's done, it's going to be a huge folder. Create a .zip and them use the\n","#Colab UI to download the .zip: cp over to  Google Drive or using the \n","#google.files download() seems to choke.\n","%cd /content/tesstrain/\n","! make training MODEL_NAME=sophonisba TESSDATA=/data/eng FINETUNE_TYPE=Plus WORD_FILE=/data/sophonisba/sophonisba.wordlist PUNC_FILE=/data/sophonisba/sophonisba.punc MAX_ITERATIONS=6000"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QdyGTtVIAN1V"},"source":["## Loading a completed training to see what Tesseract has done\n","Here's where we'll load the output for a completed Tesseract training so we can try some various things."]},{"cell_type":"markdown","metadata":{"id":"ypZjSJ_BB8rU"},"source":["### Adding a completed .traineddata file to our installation of Tesseract\n","This makes our new training available to Tesseract"]},{"cell_type":"code","metadata":{"id":"7MWq6PVMCbRm"},"source":["%cp /content/ocr_training_materials/sophonisba.traineddata /usr/share/tesseract-ocr/4.00/tessdata/sophonisba.traineddata"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HKwc6G1gCxAE"},"source":["#Get page image files from Google Drive\n","%cp -r /gdrive/MyDrive/rbs_digital_approaches_2021/data_class/page_images/penn_pr3732_t7_1730b.zip /content/penn_pr3732_t7_1730b/\n","%cd /content/\n","!unzip penn_pr3732_t7_1730b.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yhbkMJnAsYji"},"source":["#Install Python wrapper for Tesseract\n","!pip install pytesseract"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hnNY8cQGxlJE"},"source":["import pytesseract\n","from PIL import Image"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_u4HUy87GOvi"},"source":["#### How's it look?\n","Let's see what this gets us."]},{"cell_type":"code","metadata":{"id":"QAzVhxF_yCO4"},"source":["image_file = '/content/penn_pr3732_t7_1730b/bw/PR3732_T7_1730b_body0009-bw.tif'\n","im = Image.open(image_file)\n","untrained_string = pytesseract.image_to_string(im, lang='eng')\n","trained_string = pytesseract.image_to_string(im, lang='sophonisba')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZOOTht9VDx7N"},"source":["print(untrained_string)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rNG2U2RsGK9l"},"source":["print(trained_string)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8eBdSCnhGfiQ"},"source":["## Load the training output to experiment with different checkpoints\n","The training we're working with is one that I ran a few different times, varying the maximum number of iterations that the training ran.\n","\n","I say that I ran it \"a few different times,\" but really I ran it just once (at 10,000 iterations—the default) and then \"continued\" the training by picking up from different checkpoints and setting a new number of iterations. \n","\n","* I rolled back to an early checkpoint, for instance, to see how things looked with just 6,000 iterations (I read several comments about fine tuning that suggested that you could start to see good results with lower numbers of iterations—and that running too many iterations could cause the model to lose its ability to generalize. More about that in our discussion.)\n","\n","* I continued from a later checkpoint, allowing it to run for 15,000 iterations. Interestingly, when running the language on a page from Sophonisba, I saw improvement up to about 14,000 iterations, but then things got *worse* between 14,000 and 15,000.\n","\n","The next cell shows the contents of the folder of checkpoints that `testrain` produced from my training (sorted with the latest checkpoints at the top). You can copy the names of different checkpoints into the cell below that, altering the MAX_ITERATIONS variable to see what kind of effect those adjustments can have."]},{"cell_type":"code","metadata":{"id":"hAKTQiJkI4Kw"},"source":["%ls -lt /content/ocr_training_materials/sophonisba/checkpoints/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PeiSllw0j5M1"},"source":["#Copy a checkpoint from the list above and paste it over \"sophonisba_checkpoint\"\n","#(right after \"START_MODEL=\"). Then go to the end of the line and change the \n","#value of MAX_ITERATIONS. Because the training has already been run, this should \n","#usually only take a couple of mintues to re-run, depending on the values\n","#you supply\n","%cd /content/tesstrain/\n","! make training MODEL_NAME=sophonisba START_MODEL=sophonisba_checkpoint TESSDATA=/data/eng FINETUNE_TYPE=Plus WORD_FILE=/data/sophonisba/sophonisba.wordlist PUNC_FILE=/data/sophonisba/sophonisba.punc MAX_ITERATIONS=12000"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zIvq0zy0J5xI"},"source":["### Update the .traineddata in our Tesseract installation\n","For each new training you run, you'll need to move the resulting .traineddata file into our installation of Tesseract for the changes to take effect."]},{"cell_type":"code","metadata":{"id":"oPJ7AA9jKIx2"},"source":["%cp /content/tesstrain/data/sophonisba.traineddata /usr/share/tesseract-ocr/4.00/tessdata/sophonisba.traineddata"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VHr_fQMQKUiV"},"source":["### See how the output changes\n","Re-run tesseract with your new language model"]},{"cell_type":"code","metadata":{"id":"ZpUkOOVhKbTG"},"source":["new_model = pytesseract.image_to_string(im, lang='sophonisba')\n","print(new_model)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ynpx4UiGLo_n"},"source":["## Okay, but let's try it on something other than *Sophonisba*\n","Sobering."]},{"cell_type":"code","metadata":{"id":"yUi6gVjV1hXE"},"source":["bl_image = '/gdrive/MyDrive/rbs_digital_approaches_2021/data_class/page_images/bl_iiif-bw.png'\n","next_image = Image.open(bl_image)\n","next_test_string = pytesseract.image_to_string(next_image, lang='sophonisba')\n","print(next_test_string)"],"execution_count":null,"outputs":[]}]}